本文主要分享了k8s在一下科技落地的相关实践.主要从以下几点来进行讨论:

* 容器化的背景介绍
* 整体架构
* 各个模块的落地细节
* 遇到的问题


#### 容器化的背景介绍

&emsp;随着公司业务规模的不断变大, 为了能够快速迭代, 公司的后端架构也逐渐地从传统架构模型转向了微服务架构. 公司主要开发语言是Java, Golang, PHP.大部分的服务都运行在公有云的虚拟主机上.

在没有容器化之前, 我们遇到的一些主要问题:

* 流程繁琐

后端架构转成微服务以后服务的数量变得更多, 每个服务都需要开通虚拟机, 配置监控, 配置jenkins, 配置ELK, 配置白名单等.不同的开发语言有些配置还不同, 这些琐碎的事情消耗了运维的很多精力.

* 没有稳定的测试环境.

由于测试环境之间也需要互相调用, 经常联调, 由于一些历史原因, 不是所有的服务都有稳定的测试环境, 这给测试人员带来了不少的麻烦, 经常会用线上的部分机器进行调试, 存在不少的潜在风险.

* 资源利用率低.

为了方便管理, 每个服务都是分配的单独的服务器进行部署, 由于产品用户的使用习惯, 公司大部分的服务的资源使用都有高峰低估的特点, 为了保障服务的资源充足, 我们核心的服务高峰的时候也会控制资源使用率, 所以大部分服务在平时的资源使用率都是很低的, 造成了资源的大量浪费.

* 扩容/缩容不及时

业务高峰期的时候经常需要扩容机器, 扩容需要开发申请, 运维审批, 部署服务, 测试确认等很多环节, 一个服务的扩容最快也得需要半个小时才能完成, 半个小时对于很对关键服务来说其实是很长的, 是无法忍受的.

* 部署系统的不足

后端系统在向微服务演进的过程中, 不同的后端小团队出现了编程语言, 框架等的多元化, 之前存在的部署系统不能充分满足多语言多框架的部署需求.(之前大部分是PHP)


#### 整体架构

![整体架构](https://user-images.githubusercontent.com/7486508/48251512-deaf4800-e43c-11e8-9f5e-7014a924735a.png)

为了解决上面提到的问题, 同时也看到很多大公司已经在生产环境采用了k8s, 我们也把我们的服务逐渐迁移到k8s容器集群上.

#### 各个模块的落地细节

##### 容器的接入

这一部分主要跟大家分享的是我们怎样把运行在传统虚拟机/物理机上的服务逐渐接入到k8s上的.

在进行容器化改造之前, 我们的运维平台已经具有了代码发布的功能, 我们为不同编程语言的项目制定了部署规范, 比如部署路径, 日志路径, 启停方式, 回调脚本等. 每一个服务要接入部署系统都需要在平台上提工单申请,工单信息大致如下:

![image](https://user-images.githubusercontent.com/7486508/48253007-36e84900-e441-11e8-98de-3637c5b38e51.png)

通过上面的工单, 足够可以收集运维关心的服务信息, 这些信息都会记录到我们的运维平台上.容器集群搭建完成以后, 开发人员只需要为该项目申请容器部署即可, 构建的过程可以复用之前的, 不用做任何改动, 只需要在构建完成以后制作相应的镜像推送到内部的docker仓库中, 后面会详细说这些, 下面是开发申请项目接入K8s的主要选项(有分页不便截图):

* 项目名称
* 环境选择, 线上/测试
* 容器副本数量
* HPA扩容范围, 比如3-10
* 资源配置(request), 比如1核1G
* 集群内访问/负载均衡, 如果连接该项目的服务都已经部署到了容器中, 那么该服务选择集群内访问的方式就行, 也就是采用clusterIP的方式, 如果该服务是对公网的, 或者访问该服务的客户端部署在传统VM/物理机上, 这时候就需要一个负载均衡了.这里的负载均衡是公有云提供的LoadBalancer.

开发人员提交工单, 运维人员处理完工单以后这个项目基本上就可以通过部署系统部署到容器了.

##### 容器CICD

![3](https://user-images.githubusercontent.com/7486508/48297246-bf68f700-e4de-11e8-9a7c-7aa6003b8e51.png)

这里简单说一下上图的具体步骤:

1. 推送代码, 代码仓库使用的是gitlab
2. 运维平台是以项目为中心而设计开发的, 平台上的每一个 


