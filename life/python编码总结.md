title: python编码总结
date: 2015-12-21 13:33:54
tags: [python编码,python]
commons: true
categories: 编程语言
description: 编码问题是每个程序员或多或少会遇到的问题,无论是文件读写,网页解析,数据库操作等等,只要有中文的地方就会有编码的问题,python也不例外,当然网上有很多相关文章和解决办法,但是自己没有亲自总结的话就得每次遇到问题都得查,很浪费时间,所以抽空总结了一下这篇文章,来系统地掌握编码的问题.
---

## 为什么写这篇文章

&emsp;中文编码由于历史原因牵扯到不少标准,在不了解的时候就会一头雾水,每次遇到一个问题只能蒙,只能查,然后测试,成功后就不管了,很浪费时间,而且还很蛋疼,本来就不是个大问题,还浪费哥这么长时间,所以花了点时间彻底搞懂编码的来龙去脉,目标是了解编码的关键知识点,能分析和解决开发过程中碰到的大部分编码问题,这里不会过多地介绍枯燥的编码标准,尽量使用最少的语言来表达最核心的问题.这里主要基于*nux平台(Linux或者Mac系统)主要针对python2,据说python3的编码问题已经很少了.下面开始吧~

## 编码的核心问题是什么

&emsp;电脑很聪明,可以帮助我们解决很多问题,但最开始的时候是解决科学计算的问题,也就是电脑里最开始的时候只认识数字,所有的存储和计算都使用二进制表示,也就是电脑只认识0和1以及0/1间的各种运算(位运算),但是当计算机进入到人们的生活中的时候,就遇到了麻烦,因为生活中的使用场景不仅仅是数字,还有英文,标点等字符呢,那咋办呢,这个问题当然难不倒聪明的计算机科学家啦,用数字代表字符呗,这就是所谓的`编码`.

## 常见的编码/字符集介绍

&emsp;有木有发现工作中常会遇到很多很多编码格式之间的转换,如果我连基本的定义都不知道,我怎么才能熟练地在他们之间进行转换呢,所以第一步先熟悉这些基本的定义.

### 英文的终极解决方案ASCII

&emsp;上面提到了编码的核心为题是电脑不识字,所以就需要一套数字和字符之间的映射表;当然每个人都可以自己定义,比如我就可以定义'00000110'→'A',但是如果每个人都有一个标准的话那还了得,怎么进行通讯呢?所以随着时间的发展,由于计算机是老美发明的,老美在1986年制定了一个映射标准,成为`ASCII`.
&emsp;ASCII（American Standard Code for Information Interchange，美国信息交换标准代码）是基于拉丁字母的一套电脑编码系统。它主要用于显示现代英语，而其扩展版本EASCII则可以勉强显示其他西欧语言。它是现今最通用的单字节编码系统（但是有被Unicode追上的迹象），并等同于国际标准ISO/IEC 646。ASCII既是字符集又是编码方案.现在出现的各种编码都兼容并扩展了ASCII编码.

&emsp;ASCII字符集: 主要包括控制字符(回车键,退格,换行键等),可显示字符(包括英文大小写字母,阿拉伯数字,和英文符号)

&emsp;ASCII编码: 将ASCII字符集转换为计算机可接受的数字系统的数(二进制数字)的规则.使用7位(bit)表示一个字符,共128个字符;为了支持部分西欧语言的显示问题EASCII标准扩充了ASCII标准,将7位扩充到8位了,所以可以支持256个字符,EASCII码比ASCII码扩充出来的符号包括表格符号、计算符号、希腊字母和特殊的拉丁符号。

![ASCII](https://raw.githubusercontent.com/hellorocky/techblog/master/picture/binascii%E6%A8%A1%E5%9D%97%E6%80%BB%E7%BB%931.png)

![EASCII](https://raw.githubusercontent.com/hellorocky/techblog/master/picture/EASCII.gif)

&emsp;缺点: ASCII的局限在于只能显示26个基本拉丁字母、阿拉伯数目字和英式标点符号，因此只能用于显示现代美国英语（而且在处理英语当中的外来词如naïve、café、élite等等时，所有重音符号都不得不去掉，即使这样做会违反拼写规则）。而EASCII虽然解决了部分西欧语言的显示问题，但对更多其他语言依然无能为力。

### 中文编码的第一次尝试: GB2312

&emsp;ASCII这种字符编码规则处理英文当然没有问题的,但是计算机不只是老美在用啊,我们中国人也在用啊,我们使用的是中文,日本使用的是日语,韩国使用的是韩语,常用汉字6000左右,ASCII单字节(8bit)编码显然是不够用的.为了粉碎美帝国主义通过编码限制中国人民使用电脑的无耻阴谋，中国国家标准总局发布了GB2312码即中华人民共和国国家汉字信息交换用编码，全称《信息交换用汉字编码字符集——基本集》，1981年5月1日实施，通行于大陆。GB 2312是一个简体中文字符集，由6763个常用汉字和682个全角的非汉字字符组成。著作权归作者所有。GB2312只是一个区位码形式的字符集标准，不过实际上基本都用`EUC-CN`来编码，所以提及`GB2312`时也说的是一个字符集和编码连锁的方案；`GBK`和`GB18030`等向后兼容于 `GB2312`的方案也类似。

&emsp;传统上，英语或拉丁字母语言使用的电脑系统，每一个字母或符号，都是使用一字节的空间（一字节由8比特组成，共256个编码空间）来储存；而汉语、日语及韩语文字，由于数量大大超过256个，故惯常使用两字节来储存一个字符，在使用固定宽度文字的地方（如DOS、部分文字编辑器等），为了使字体看起来齐整，英文字母、数字及其他符号，也由原来只占一个字空间，改为一概占用两个字的空间来显示，并且使用两个字节来储存。所以，中、日、韩等文字称为全角字符，相比起来，拉丁字母或数字就称为半角字符。

&emsp;GB2312编码用两个字节(8位2进制)表示一个汉字，所以理论上最多可以表示256×256=65536个汉字。但这种编码方式也仅仅在中国行得通，如果您的网页使用的GB2312编码，那么很多外国人在浏览你的网页时就可能无法正常显示，因为其浏览器不支持GB2312编码。当然，中国人在浏览外国网页(比如日文)时，也会出现乱码或无法打开的情况，因为我们的浏览器没有安装日文的编码表。

### 中文编码的第二次尝试: GBK

&emsp;GB2312的出现基本满足了汉字的计算机处理需要，但由于上面提到未收录繁体字和生僻字，从而不能处理人名、古汉语等方面出现的罕用字，这导致了1995年《汉字编码扩展规范》（GBK）的出现。GBK编码是GB2312编码的超集，向下完全兼容GB2312，兼容的含义是不仅字符兼容，而且相同字符的编码也相同，同时在字汇一级支持ISO/IEC10646—1和GB 13000—1的全部中、日、韩（CJK）汉字，共计20902字。GBK还收录了GB2312不包含的汉字部首符号、竖排标点符号等字符。CP936和GBK的有些许差别，绝大多数情况下可以把CP936当作GBK的别名。

### 中文编码的第三次尝试: GB18030

&emsp;GB18030编码向下兼容GBK和GB2312。GB18030收录了所有Unicode3.1中的字符，包括中国少数民族字符，GBK不支持的韩文字符等等，也可以说是世界大多民族的文字符号都被收录在内.其实，这三个标准并不需要死记硬背，只需要了解是根据应用需求不断扩展编码范围即可。从GB2312到GBK再到GB18030收录的字符越来越多即可。万幸的是一直是向下兼容的，也就是说一个汉字在这三个编码标准里的编码是一模一样的。这些编码的共性是变长编码，单字节ASCII兼容，对其他字符GB2312和GBK都使用双字节等宽编码，只有GB18030还有四字节编码的方式。

&emsp;最后，提一个小插曲，上面讲的都是大陆推行的汉字编码标准，使用繁体的中文社群中最常用的电脑汉字字符集标准叫大五码（Big5),是港澳台等地的标准,工作中基本用不到.

### 伟大的创想unicode

&emsp;看了上面的多个中文编码是不是有点头晕了呢？如果把这个问题放到全世界n多个国家n多语种呢？各国和各地区自己的文字编码规则互相冲突的情况全球信息交换带来了很大的麻烦。

&emsp;要真正彻底解决这个问题，上面介绍的那些通过扩展ASCII修修补补的方式已经走不通了，而必须有一个全新的编码系统，这个系统要可以将中文、日文、法文、德文……等等所有的文字统一起来考虑，为每一个文字都分配一个单独的编码。于是，Unicode诞生了。Unicode（统一码、万国码、单一码）为地球上（以后会包括火星，金星，喵星等）每种语言中的每个字符设定了统一并且唯一的二进制编码，以满足跨语言、跨平台进行文本转换、处理的要求。在Unicode里，所有的字符被一视同仁，汉字不再使用“两个扩展ASCII”，而是使用“1个Unicode”来表示，也就是说，所有的文字都按一个字符来处理，它们都有一个唯一的Unicode码。对于`Unicode`，字符集和编码是明确区分的。Unicode/UCS标准首先是个统一的字符集标准。而`unicode/UCS`标准同时也定义了几种可选的编码方案，在标准文档中称作`encoding form`，主要包括`UTF-8/UTF-16/UTF-32`。所以，对`Unicode`方案来说，同样的基于`Unicode`字符集的文本可以用多种编码来存储、传输。

> The rules for translating a Unicode string into a sequence of bytes are called an encoding.

Unicode的特点以及它和UTF-8的关系:

* unicode是一个字符集,也就是一种字符和二进制的映射关系表,它包括目前所有的字符(日语,汉语,韩语等等)但是unicode并没有规定如何存储二进制数字,和如何传输.UTF(UCS Transformation Format)规定了.

* UTF-8使用3个字节存放一个汉字

* unicode是不能直接保存到文件的，存到文件系统的只能是二进制，由于二进制肯定是某种编码的二进制，所以读取的时候要对上号

为什么会需要UTF-8编码的存在呢?

* 统一字库表的目的是为了能够涵盖世界上所有的字符，但实际使用过程中会发现真正用的上的字符相对整个字库表来说比例非常低。例如中文地区的程序几乎不会需要日语字符，而一些英语国家甚至简单的ASCII字库表就能满足基本需求。而如果把每个字符都用字库表中的序号来存储的话，每个字符就需要3个字节（这里以Unicode字库为例），这样对于原本用仅占一个字符的ASCII编码的英语地区国家显然是一个额外成本（存储体积是原来的三倍）。算的直接一些，同样一块硬盘，用ASCII可以存1500篇文章，而用3字节Unicode序号存储只能存500篇。于是就出现了UTF-8这样的变长编码。在UTF-8编码中原本只需要一个字节的ASCII字符，仍然只占一个字节。而像中文及日语这样的复杂字符就需要2个到3个字节来存储。简单的说，实际用到的概念主要是`字符集`和`编码方式`。其中字符集决定了我们所能使用的字符，例如，当使用`ASCII`字符集的时候，不能表示中文。编码方式则决定了这些字符在磁盘上的存储方式，如对于`Unicode`字符集，我可以选择`UTF-8、UTF-16、UTF-32`中的一种方式进行存储。

## python中的编码问题

* python编码支持情况

&emsp;Python本身对各种语言支持都很好,字符串在python内部是用Unicode编码表示的.python支持两种标准的Unicode,一个是UCS-2,另一个是UCS-4,可以通过以下方法查看,当然编译的时候可以更改:
```
>>>import sys
>>> print sys.maxunicode
65535 #UCS-2
1114111 #UCS-4
```

*编码转化

&emsp;如果需要做编码转换,需要借助内部编码(Unicode)作为中转来转换,通常过程是这样的:
```
原编码 → decode → 内部编码(Unicode) → 目标编码

```
Linux/Mac系统的默认编码为UTF-8,python语言的默认编码为ASCII:
```
#Mac/Linux
>>> import sys
>>> sys.getfilesystemencoding()
'utf-8'

#python interpreter
>>>import sys
>>>print sys.getdefaultencoding()
'ascii'

```
举几个例子:

```
在Linux终端下:
>>>a = '吴飞群'
>>>a
>>>'\xe5\x90\xb4\xe9\xa3\x9e\xe7\xbe\xa4'
>>>a.decode()
Traceback (most recent call last):
  File "<stdin>", line 1, in <module>
UnicodeDecodeError: 'ascii' codec can't decode byte 0xe5 in position 0: ordinal not in range(128)

会有上面的报错,因为系统默认编码为UTF-8,但是python默认编码为ascii,str.decode()如果不加参数的话,就会使用python默认的编码方式去把该字符串解码成为Unicode,因为ASCII不支持中文所以报如上的错误;

>>>a.decode('gb2312')
Traceback (most recent call last):
  File "<stdin>", line 1, in <module>
UnicodeDecodeError: 'gb2312' codec can't decode bytes in position 0-1: illegal multibyte sequence

类似错误,GB2312编码不能解码UTF-8的字符,不同的编码方法是不可以相互解码的.

>>>a.decode('utf-8')
u'\u5434\u98de\u7fa4'

这次成功了.
当然我们也可以把python默认的编码方式改为UTF-8:
>>>import sys
>>>reload(sys)
>>>sys.setdefaultencoding('utf-8')

很多情况下我们并不知道某个字符是什么编码,那该怎么办呢?哈哈,万能的python,使用第三方库chardet,安装这里就不说了.
使用:
>>>a = '你好'
>>>import chardet
>>>chardet.detect(a)
{'confidence': 0.7525, 'encoding': 'utf-8'}

有些时候需要读取/写入文本,需要做一些编码转换:
>>>open('1.txt', 'wb').write(u'rockywu')
>>>import chardet
>>>a = open('1.txt', 'rb').read()
>>>chardet.detect(a)
{'confidence': 1.0, 'encoding': 'ascii'}
这说明如果直接把Unicode类型字符写入文件的话,python会用默认的编码方式先编码后写入.
codecs



python内置ASCII相关方法:
ord('a') #→97
chr(97) #->a

str和Unicode相加:
>>>u'rocky' + 'wu'
u'rockywu'



```






## 常见设置方法

&emsp;在计算机内存中(python内部)，统一使用Unicode编码，当需要保存到硬盘或者需要传输的时候，就转换为UTF-8编码。

* .py文件
```
#!coding=utf-8
```

## 其它相关编码

&emsp;这里简单介绍一下`base64`和URL编码.

* 什么是base64?

Base64是一种基于64个可打印字符[0-9a-zA-z+/]来表示二进制数据的表示方法。base64就是编码算法而已，做的事就是把三个八位字节转换为4个6位字节，然后做一些补全等操作,是一种对称加密算法.

*目的

Base64主要用于将不可打印的字符转换成可打印字符，或者简单的说将二进制数据编码成ASCII字符。将二进制数据编码成ASCII字符主要的目的是能在纯文本内容中插入二进制数据，比如图片,视频等等,常见的场景就是email的传输了,因为email的协议只支持文本.(待验证)

* 特点

 1. 编码后的数据大小比之前变大,大概是之前的4/3.

* 常用方法

```
Linux平台下:
加密方法:
#echo '吴飞群'|base64
#5ZC06aOe576kCg==
解密方法:
#echo '5ZC06aOe576kCg=='|base64 -d
#吴飞群
```

---

* URL编码

url的编码也是UTF-8的,只不过在每个16进制的数字之间都有一个`%`;举几个例子:
```
'你好'的UTF-8编码为:'\xe4\xbd\xa0\xe5\xa5\xbd'(\x是十六进制的表示方式)
URL输入框显示:https://www.zhihu.com/search?type=question&q=你好
实际请求URL:https://www.zhihu.com/search?type=question&q=%E4%BD%A0%E5%A5%BD
URL输入框显示:https://www.google.com/#newwindow=1&q=%E4%BD%A0%E5%A5%BD
实际请求URL:https://www.google.com/#newwindow=1&q=%E4%BD%A0%E5%A5%BD

```

