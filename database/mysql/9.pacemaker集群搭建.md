#### 背景介绍

&emsp;公司数据库从之前的主从架构切换到了Galera集群,前端还需要一个网络层的高可用方案,可以使用F5,但由于公司的架构问题,后来打算使用了虚IP的方式来处理,正好有机会练练手了,根据公司同时的经验选择了pacemaker,这一篇文章说的是pacemaker的基本基本搭建过程.

#### 环境准备

| HostName      | IP            |
| ------------- | ------------- |
| test1  | 172.100.102.151      |
| test3  | 172.100.102.155      |
| test5  | 172.100.102.160      |

* 操作系统: CentOS 6.X
* 所需软件:
 * pacemaker-1.1.14 (直接安装即可, 资源管理,比如虚IP就是一种资源,当然硬盘,CPU等都是资源)
 * corosync-1.4.7 (直接安装即可, 节点间的心跳)
 * crmsh-2.3.2 (pacemaker命令行控制工具,当然也可以使用其它的比如pcs,额外下载安装3个软件crmsh, crmsh-scripts, python-parallax(先安装))
 * pssh 


#### 软件配置

&emsp;按照上面的方法就可以安装好了,下面开始配置软件,这里需要注意,我们只需要配置corosync即可,pacemaker以插件的方式启动,不需要专门来配置,也不需要单独手工启动,这里只需要关心corosync即可.先配置hosts文件,不同节点可以把不同的IP改成current-node



* hosts配置,注意每个节点都把current-node修改成自己的节点:

```bash
172.100.102.151 test1 current-node
172.100.102.155 test3
172.100.102.160 test5
```


* corosync配置:

```bash
# Please read the corosync.conf.5 manual page
compatibility: whitetank

totem {
	version: 2
	secauth: off
	interface {
		member {
			memberaddr: test1
		}
		member {
			memberaddr: test3
		}
		member {
			memberaddr: test5
		}
		ringnumber: 0
		bindnetaddr: current-node
		mcastport: 5405
		ttl: 1
	}
	transport: udpu
}

logging {
	fileline: off
	to_logfile: yes
	to_syslog: yes
	logfile: /var/log/cluster/corosync.log
	debug: off
	timestamp: on
	logger_subsys {
		subsys: AMF
		debug: off
	}
}

amf {
    mode: disabled
}
#启用pacemaker
service {
    ver: 1 # 0代表自动拉起来服务,1的话不会自动拉起来,使用1的时候注意把corosync和pacemaker加入开机启动项
    name: pacemaker
}
aisexec {
    user: root
    group: root
}
```

服务起来后会监听UDP端口5405,可以使用如下方式测试是否通, 正常情况下会阻塞,并且没有报错,如果有报错则代表节点间通信有问题:

```bash
[root@test ~]# netstat -anup|grep 5405
udp        0      0 192.10.30.94:5405           0.0.0.0:*                               16019/corosync

```

* 接下来测试集群是否成功:

```bash
[root@test3 ~]# crm status 
Last updated: Mon Jun 30 12:47:53 2014 
Last change: Mon Jun 30 12:47:39 2014 via crmd on node2 
Stack: classic openais (with plugin) 
Current DC: node2 - partition with quorum Version: 1.1.10-14.el6_5.3-368c726 
3 Nodes configured, 3 expected votes 0 Resources configured 
Online: [ test1 test2 test3 ] 
```

或者使用`crm_mon`,如上图就代表配置成功了!

* 添加开机启动

```bash
[root@test3 ~]#chkconfig  corosync on
[root@test3 ~]#chkconfig  pacemaker on
查看是否加入开机启动:
[root@test3 ~]# chkconfig --list |grep corosync
corosync       	0:关闭	1:关闭	2:启用	3:启用	4:启用	5:启用	6:关闭

```

* 设置一些默认属性, 在其中一台机器上执行即可

```bash
[root@test3 ~]# crm configure property 'stonith-enabled'='false'
```

* 配置虚IP

```bash
[root@test01 ~]# crm configure primitive ClusterIP ocf:heartbeat:IPaddr2 \
params ip=172.100.102.110 nic="eth0:0" cidr_netmask=24 \
 op monitor interval=30s \
meta target-role=Started

解释:
ClusterIP 是资源名称,pacemaker把一切当做资源,比如虚拟IP,硬盘等等
IPaddr2 关键字,代表虚IP资源

```

检查配置文件是否正确

```bash
crm(live)# verify -L
Current cluster status:
Online: [ test1 test3 test5 ]

 test_resource	(ocf::heartbeat:IPaddr2):	Started test3
```

* 查看配置:

```bash
[root@test1]# crm configure show
node test1
node test3
node test5
primitive test_resource IPaddr2 \
	params ip=172.100.102.110 nic="eth0:0" cidr_netmask=24 \
	op monitor interval=30s \
	meta target-role=Started
location cli-prefer-test_resource test_resource role=Started -inf: test5
property cib-bootstrap-options: \
	dc-version=1.1.14-8.el6_8.2-70404b0 \
	cluster-infrastructure="classic openais (with plugin)" \
	expected-quorum-votes=3 \
	stonith-enabled=false
```

* 手工指定节点

一般来说,资源是根据投票来决定指向哪一个节点的,当然我们也可以手工指定节点,如下实例:

```
[root@test3 ~]# crm
crm(live)# status
Last updated: Thu Jan  5 10:54:15 2017		Last change: Thu Jan  5 10:51:55 2017 by root via crm_resource on test3
Stack: classic openais (with plugin)
Current DC: test5 (version 1.1.14-8.el6_8.2-70404b0) - partition with quorum
3 nodes and 1 resource configured, 3 expected votes

Online: [ test1 test3 test5 ]

Full list of resources:

 test_resource	(ocf::heartbeat:IPaddr2):	Started test1

```

目前虚IP资源在节点test1上,我来给手工指定到test3上,crmsh命令行太好用了,双击tab可以自动命令补全,也可以列出全部可用的命令,不知道命令怎么使用的时候直接敲一个`help`即可:

```
[root@test3 ~]# crm
crm(live)#
--help        back          cib           configure     exit          ls            options       report        site          verify
-h            bye           cibstatus     corosync      help          maintenance   quit          resource      status
?             cd            cluster       end           history       node          ra            script        up
crm(live)# resource
crm(live)resource#
--help        bye           demote        list          meta          promote       scores        stop          unmove
-h            cd            end           locate        migrate       quit          secret        trace         untrace
?             cleanup       exit          ls            move          refresh       show          unban         up
back          clear         failcount     maintenance   operations    reprobe       start         unmanage      utilization
ban           constraints   help          manage        param         restart       status        unmigrate
crm(live)resource# move test_resource test3
INFO: Move constraint created for test_resource to test3
crm(live)resource# status
 test_resource	(ocf::heartbeat:IPaddr2):	Started
crm(live)resource# exit
bye
[root@test3 ~]# crm status
Last updated: Thu Jan  5 10:56:08 2017		Last change: Thu Jan  5 10:55:51 2017 by root via crm_resource on test3
Stack: classic openais (with plugin)
Current DC: test5 (version 1.1.14-8.el6_8.2-70404b0) - partition with quorum
3 nodes and 1 resource configured, 3 expected votes

Online: [ test1 test3 test5 ]

Full list of resources:

 test_resource	(ocf::heartbeat:IPaddr2):	Started test3



```

手工启动节点方法:

```
当某一个节点没有起来的时候,一般会自动加入,如果没有加入集群的话,我们可以手工把那个节点加入到集群里面:

crm(live)# node
crm(live)node# online test1
crm(live)node# online test3
然后查看状态即可
```

* 资源粘性

我们经常会遇到这样一种情况,有A,B,C三个节点,目前主节点在A,当A挂了后,主节点会到B或C上,我们想让A恢复后让主节点继续在A上,这时候就需要设置A的粘性值更大点,所有的节点的默认值是0,如下是配置方式:

```
值为 0：这是默认选项。资源放置在系统中的最适合位置。这意味着当负载能力“较好”或较差的节点变得可用时才转移资源。此选项的作用几乎等同于自动故障回复，只是资源可能会转移到非之前活动的节点上。
值大于 0：资源更愿意留在当前位置，但是如果有更合适的节点可用时会移动。值越高表示资源越愿意留在当前位置。
值小于 0：资源更愿意移离当前位置。绝对值越高表示资源越愿意离开当前位置。
值为 INFINITY：如果不是因节点不适合运行资源（节点关机、节点待机、达到migration-threshold 或配置更改）而强制资源转移，资源总是留在当前位置。此选项的作用几乎等同于完全禁用自动故障回复。
值为 -INFINITY：资源总是移离当前位置。

```

Pacaemaker/corosync allows resource to choose the preferred location.location的概念我理解的是,pacemaker是资源管理器,它可以管理资源让资源选择经过投票决出的合适的节点,我们配置资源粘性就可以通过某个资源的location属性来让资源更倾向在哪个节点上

```bash
# crm configure location ClusterIP-prefer ClusterIP inf: test3
```

然后查看一下配置文件:

```bash
[root@test3 ~]# crm configure show
node test1 \
	attributes standby=off
node test3 \
	attributes standby=off
node test5 \
	attributes standby=off
primitive ClusterIP IPaddr2 \
	params ip=172.100.102.110 nic="eth0:0" cidr_netmask=24 \
	op monitor interval=30s \
	meta target-role=Started
location clusterIP-preffer ClusterIP inf: test3
property cib-bootstrap-options: \
	dc-version=1.1.14-8.el6_8.2-70404b0 \
	cluster-infrastructure="classic openais (with plugin)" \
	expected-quorum-votes=3 \
	stonith-enabled=false \
	cluster-recheck-interval=1m \
	last-lrm-refresh=1483586538 \
	cluster-delay=5s

```

* 删除资源

删除前先停止运行的资源:

```bash
#crm resource stop test_resource
#crm configure delete test_resource

test_resource 是资源的名称
```

* `cluster-delay`参数的设置

在测试pacemaker的过程中遇到了这样的一个问题,使用`kill -9`的方式杀掉其中一个节点的pacemaker/corosync服务后,然后再启动,发现需要等待很长时间该节点才能正常加入到集群中,这时候发现了这个参数,默认是`60s`,后来改成了`5s`后恢复起来快多了,配置这个参数的方式可以使用vim的方式:

```bash
[root@test3 ~]# crm configure edit

node test1 \
        attributes standby=off
node test3 \
        attributes standby=off
node test5 \
        attributes standby=off
primitive ClusterIP IPaddr2 \
        params ip=172.100.102.110 nic="eth0:0" cidr_netmask=24 \
        op monitor interval=30s \
        meta target-role=Started
location clusterIP-preffer ClusterIP inf: test3
property cib-bootstrap-options: \
        dc-version=1.1.14-8.el6_8.2-70404b0 \
        cluster-infrastructure="classic openais (with plugin)" \
        expected-quorum-votes=3 \
        stonith-enabled=false \
        cluster-recheck-interval=1m \
        last-lrm-refresh=1483586538 \
        cluster-delay=5s
        
```

如上,把该参数添加到左后,然后提交更改并验证配置文件

```bash
[root@test3 ~]# crm configure commit    
[root@test3 ~]# crm_verify -L   # 动态检查配置文件的正确性,如果正确,没有返回,如果配置文件有错误,那么就会抛出异常
```

没有报错的话就代表正常!


#### 让pacemaker管理maxscale服务

&emsp;到上面这一步,我们已经配置好了虚IP了,而且当虚IP所在的节点出问题的时候比如机器挂了,或者corosync服务停止了,虚IP或自动切换,但是问题是我们除了检查这些很基础的监控外还的看我们的核心服务maxscale啊,我们必须有能力根据maxscale服务是否正常运行来确定虚IP在哪个节点上,所以这里需要把IP资源和maxscale服务资源绑定在一个组里,因为pacemaker管理服务有两种方式一种是lsb,就是我们经常使用的`service xxx status`类似这种方式的标准来检查服务的状态,并根据命令执行后的返回码来判断服务是否正常,使用`echo $?`可以看到上次命令执行的成功与否,`0`代表成功,`1`代表不成功.

&emsp;由于公司所有的服务使用`supervisor`来管理的,而没有采用LSB的方式来管理,所以还需要自己实现一个脚本来模拟成LSB的方式来管理,脚本地址在文章最下面有,把该脚本放到`/etc/init.d/`下面,然后通过`service maxscale status`来测试一下.下面来添加maxscale资源到pacemaker中:

```bash
primitive MaxScale lsb:maxscale \
	op monitor interval=30s timeout=15s \
	op start interval=0 timeout=15s on-fail=standby \
	op stop interval=0 timeout=30s \
	meta migration-threshold=3 target-role=Started

解释:
MaxScale    为这个配置起一个名字
lsb:maxscale    代表所监控的服务的启动方式LSB标准,服务名称为maxscale,也就是service maxscale status中的maxscale
on-faile=standby    pacemaker每30秒会检测maxscale的状态,如果该节点失败次数超过3次,就把这个节点提出集群,也就是变成standby状态.


```

让pacemaker管理maxscale,我这里的思路是这样的,3个节点的maxscale服务都正常启动,配置一个节点拥有更高的粘性,让虚IP漂在该节点上,如果发现该节点的maxscale服务出问题就把虚IP漂向根据投票投向其它可用节点,因为默pacemaker支持的方式不是这样的,而是虚IP飘在哪个节点那个节点的服务正常启动其他节点的服务是停止状态的,等待虚IP飘到那个节点那个节点的服务启动,另外两个停止状态,为了让3台节点的服务默认都正常启动,我们可以使用克隆的方式,如果某一个节点出问题,那么就让该节点处于standby状态,也就是暂时推出集群,等问题修复以后可以手工把这个节点加入集群.

```bash
primitive ClusterIP IPaddr2 \
	params ip=172.100.102.110 nic="eth0:0" cidr_netmask=24 \
	op monitor interval=30s \
	meta target-role=Started
	
primitive MaxScale lsb:maxscale \
	op monitor interval=30s timeout=15s \
	op start interval=0 timeout=15s on-fail=standby \
	op stop interval=0 timeout=30s \
	meta migration-threshold=3
	
location clusterIP-preffer ClusterIP  inf: test3

clone clone_max MaxScale

```

下面测试一发吧:

* 目前的状态:

```bash
[root@test3 ~]# crm status
Last updated: Tue Jan 10 11:15:13 2017		Last change: Tue Jan 10 11:13:14 2017 by hacluster via crmd on test5
Stack: classic openais (with plugin)
Current DC: test5 (version 1.1.14-8.el6_8.2-70404b0) - partition with quorum
3 nodes and 7 resources configured, 3 expected votes

Online: [ test1 test3 test5 ]

Full list of resources:

 Clone Set: colone_mon [ClusterMon]
     Started: [ test1 test3 test5 ]
 ClusterIP	(ocf::heartbeat:IPaddr2):	Started test3
 Clone Set: clone_max [MaxScale]
     Started: [ test1 test3 test5 ]


注解:
7 resources configured 表示虚IP代表1个资源,因为它只在一个节点上运行, clone_max是maxscale服务克隆到3台机器上运行,代表3个,colone_mon是3台机器上同时运行的监控集群的程序,也是3个资源,总共7个资源
```

* 由于我们设置`test3`节点的资源粘性更大,所以只要该节点正常,虚IP会一直运行在该节点上

```bash
[root@test3 ~]# cd /home/qfpay/maxscale/bin/
[root@test3 bin]# ls
maxadmin  maxbinlogcheck  maxkeys  maxpasswd  maxscale
[root@test3 bin]# mv maxscale{,.1}
[root@test3 bin]# ls
maxadmin  maxbinlogcheck  maxkeys  maxpasswd  maxscale.1
[root@test3 bin]# ps -ef|grep maxscale
qfpay     8633 23462  0 11:04 ?        00:00:00 /home/qfpay/maxscale/bin/maxscale -f /home/qfpay/maxscale/etc/maxscale.cnf -d
root     10264 23861  0 11:24 pts/0    00:00:00 grep maxscale
[root@test3 bin]# kill 8633
[root@test3 bin]# ps -ef|grep maxscale
root     10320 23861  0 11:24 pts/0    00:00:00 grep maxscale
```

* 然后再看看现在的集群状态:

```bash
[root@test3 bin]# crm status
Last updated: Tue Jan 10 11:25:10 2017		Last change: Tue Jan 10 11:25:01 2017 by hacluster via crmd on test5
Stack: classic openais (with plugin)
Current DC: test5 (version 1.1.14-8.el6_8.2-70404b0) - partition with quorum
3 nodes and 7 resources configured, 3 expected votes

Node test3: standby (on-fail)
Online: [ test1 test5 ]

Full list of resources:

 Clone Set: colone_mon [ClusterMon]
     Started: [ test1 test5 ]
     Stopped: [ test3 ]
 ClusterIP	(ocf::heartbeat:IPaddr2):	Started test1
 Clone Set: clone_max [MaxScale]
     Started: [ test1 test5 ]
     Stopped: [ test3 ]

Failed Actions:
* MaxScale_start_0 on test3 'unknown error' (1): call=214, status=complete, exitreason='none',
    last-rc-change='Tue Jan 10 11:24:56 2017', queued=0ms, exec=381ms

```

可以看虚IP跑到了`test1`上面了

* 恢复

```bash
[root@test3 bin]# mv maxscale.1 maxscale
[root@test3 bin]# service maxscale restart
Checking MaxScale Status...
Restarting MaxScale...
MaxScale Started...
[root@test3 bin]# service maxscale status
Checking MaxScale Status...
MaxScale is RUNNING...

[root@test3 bin]# crm status
Last updated: Tue Jan 10 11:27:50 2017		Last change: Tue Jan 10 11:25:01 2017 by hacluster via crmd on test5
Stack: classic openais (with plugin)
Current DC: test5 (version 1.1.14-8.el6_8.2-70404b0) - partition with quorum
3 nodes and 7 resources configured, 3 expected votes

Node test3: standby (on-fail)
Online: [ test1 test5 ]

Full list of resources:

 Clone Set: colone_mon [ClusterMon]
     Started: [ test1 test5 ]
     Stopped: [ test3 ]
 ClusterIP	(ocf::heartbeat:IPaddr2):	Started test1
 Clone Set: clone_max [MaxScale]
     Started: [ test1 test5 ]
     Stopped: [ test3 ]

Failed Actions:
* MaxScale_start_0 on test3 'unknown error' (1): call=214, status=complete, exitreason='none',
    last-rc-change='Tue Jan 10 11:24:56 2017', queued=0ms, exec=381ms

[root@test3 bin]# crm resource cleanup clone_max
Cleaning up MaxScale:0 on test1, removing fail-count-MaxScale
Cleaning up MaxScale:0 on test3, removing fail-count-MaxScale
Cleaning up MaxScale:0 on test5, removing fail-count-MaxScale
Waiting for 3 replies from the CRMd... OK
[root@test3 bin]# crm resource cleanup colone_mon
Cleaning up ClusterMon:0 on test1, removing fail-count-ClusterMon
Cleaning up ClusterMon:0 on test3, removing fail-count-ClusterMon
Cleaning up ClusterMon:0 on test5, removing fail-count-ClusterMon
Waiting for 3 replies from the CRMd... OK
[root@test3 bin]# crm status
Last updated: Tue Jan 10 11:28:26 2017		Last change: Tue Jan 10 11:28:23 2017 by hacluster via crmd on test5
Stack: classic openais (with plugin)
Current DC: test5 (version 1.1.14-8.el6_8.2-70404b0) - partition with quorum
3 nodes and 7 resources configured, 3 expected votes

Online: [ test1 test3 test5 ]

Full list of resources:

 Clone Set: colone_mon [ClusterMon]
     Started: [ test1 test3 test5 ]
 ClusterIP	(ocf::heartbeat:IPaddr2):	Started test3
 Clone Set: clone_max [MaxScale]
     Started: [ test1 test3 test5 ]
```

* 架构图

![架构图](https://raw.githubusercontent.com/hellorocky/blog/master/picture/17.qfpay_mariadb_cluster_arch.png)

如上面所示,使用`crm resouce cleanup resource_name`即可恢复,到此实验结束!
#### 注意事项

* 集群的所有节点必须在同一个子网内
* 配置各个节点的NTP时间同步




#### 参考链接

[crmsh](https://github.com/ClusterLabs/crmsh)

[pacemaker](https://github.com/ClusterLabs/pacemaker)

[maxscale LSB 脚本](https://github.com/hellorocky/blog/blob/master/database/mysql/maxscale)
