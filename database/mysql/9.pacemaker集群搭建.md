#### 背景介绍

&emsp;公司数据库从之前的主从架构切换到了Galera集群,前端还需要一个网络层的高可用方案,可以使用F5,但由于公司的架构问题,后来打算使用了虚IP的方式来处理,正好有机会练练手了,根据公司同时的经验选择了pacemaker,这一篇文章说的是pacemaker的基本基本搭建过程.

#### 环境准备

| HostName      | IP            |
| ------------- | ------------- |
| test1  | 172.100.102.151      |
| test3  | 172.100.102.155      |
| test5  | 172.100.102.160      |

* 操作系统: CentOS 6.X
* 所需软件:
 * pacemaker-1.1.14 (直接安装即可, 资源管理,比如虚IP就是一种资源,当然硬盘,CPU等都是资源)
 * corosync-1.4.7 (直接安装即可, 节点间的心跳)
 * crmsh-2.3.2 (额外下载安装3个软件crmsh, crmsh-scripts, python-parallax(先安装))
 * pssh 


#### 软件配置

&emsp;按照上面的方法就可以安装好了,下面开始配置软件,这里需要注意,我们只需要配置corosync即可,pacemaker以插件的方式启动,不需要专门来配置,也不需要单独手工启动,这里只需要关心corosync即可.先配置hosts文件,不同节点可以把不同的IP改成current-node



* hosts配置:

```bash
172.100.102.151 test1 current-node
172.100.102.155 test3
172.100.102.160 test5
```

* 对于corosync而言，我们各节点之间通信时必须要能够实现安全认证的，要用到一个密钥文件：

```bash
#corosync-keygen    # 生成密钥文件，用于双机通信互信，会生成authkey的文件,在/etc/corosync/目录下,生成好以后拷贝到其它另外两台机器上
```

* corosync配置:

```bash
# Please read the corosync.conf.5 manual page
compatibility: whitetank

totem {
	version: 2
	secauth: off
	interface {
		member {
			memberaddr: test1
		}
		member {
			memberaddr: test3
		}
		member {
			memberaddr: test5
		}
		ringnumber: 0
		bindnetaddr: current-node
		mcastport: 5405
		ttl: 1
	}
	transport: udpu
}

logging {
	fileline: off
	to_logfile: yes
	to_syslog: yes
	logfile: /var/log/cluster/corosync.log
	debug: off
	timestamp: on
	logger_subsys {
		subsys: AMF
		debug: off
	}
}

amf {
    mode: disabled
}
#启用pacemaker
service {
    ver: 0 # 0代表自动拉起来服务,1的话不会自动拉起来,使用0就行
    name: pacemaker
}
aisexec {
    user: root
    group: root
}
```

服务起来后会监听UDP端口5405,可以使用如下方式测试是否通, 正常情况下会阻塞,并且没有报错,如果有报错则代表节点间通信有问题:

```bash
$echo "check ..." | nc -u node1 5405

```

* 接下来测试集群是否成功:
检查配置文件是否正确

```bash
crm(live)# verify
Current cluster status:
Online: [ test1 test3 test5 ]

 test_resource	(ocf::heartbeat:IPaddr2):	Started test3
```



```bash
[root@test3 ~]# crm status 
Last updated: Mon Jun 30 12:47:53 2014 
Last change: Mon Jun 30 12:47:39 2014 via crmd on node2 
Stack: classic openais (with plugin) 
Current DC: node2 - partition with quorum Version: 1.1.10-14.el6_5.3-368c726 
3 Nodes configured, 3 expected votes 0 Resources configured 
Online: [ test1 test2 test3 ] 
```

或者使用`crm_mon`,如上图就代表配置成功了!

* 添加开机启动

```bash
[root@test3 ~]#chkconfig  corosync on
查看是否加入开机启动:
[root@test3 ~]# chkconfig --list |grep corosync
corosync       	0:关闭	1:关闭	2:启用	3:启用	4:启用	5:启用	6:关闭
```

* 设置一些默认属性, 在其中一台机器上执行即可

```bash
[root@test3 ~]# crm configure property 'stonith-enabled'='false'
[root@test3 ~]# crm configure property 'no-quorum-policy'='ignore'
```

* 配置虚IP

```bash
[root@test01 ~]# crm configure primitive ClusterIP ocf:heartbeat:IPaddr2 \
params ip=172.100.102.110 nic="eth0:0" cidr_netmask=24 \
 op monitor interval=30s \
meta target-role=Started

解释:
ClusterIP 是资源名称,pacemaker把一切当做资源,比如虚拟IP,硬盘等等
IPaddr2 关键字,代表虚IP资源



```


* 查看配置:

```bash
[root@test1]# crm configure show
node test1
node test3
node test5
primitive test_resource IPaddr2 \
	params ip=172.100.102.110 nic="eth0:0" cidr_netmask=24 \
	op monitor interval=30s \
	meta target-role=Started
location cli-prefer-test_resource test_resource role=Started -inf: test5
property cib-bootstrap-options: \
	dc-version=1.1.14-8.el6_8.2-70404b0 \
	cluster-infrastructure="classic openais (with plugin)" \
	expected-quorum-votes=3 \
	stonith-enabled=false
```

* 手工指定节点

一般来说,资源是根据投票来决定指向哪一个节点的,当然我们也可以手工指定节点,如下实例:

```
[root@test3 ~]# crm
crm(live)# status
Last updated: Thu Jan  5 10:54:15 2017		Last change: Thu Jan  5 10:51:55 2017 by root via crm_resource on test3
Stack: classic openais (with plugin)
Current DC: test5 (version 1.1.14-8.el6_8.2-70404b0) - partition with quorum
3 nodes and 1 resource configured, 3 expected votes

Online: [ test1 test3 test5 ]

Full list of resources:

 test_resource	(ocf::heartbeat:IPaddr2):	Started test1

```

目前虚IP资源在节点test1上,我来给手工指定到test3上,crmsh命令行太好用了,双击tab可以自动命令补全,也可以列出全部可用的命令,不知道命令怎么使用的时候直接敲一个`help`即可:

```
[root@test3 ~]# crm
crm(live)#
--help        back          cib           configure     exit          ls            options       report        site          verify
-h            bye           cibstatus     corosync      help          maintenance   quit          resource      status
?             cd            cluster       end           history       node          ra            script        up
crm(live)# resource
crm(live)resource#
--help        bye           demote        list          meta          promote       scores        stop          unmove
-h            cd            end           locate        migrate       quit          secret        trace         untrace
?             cleanup       exit          ls            move          refresh       show          unban         up
back          clear         failcount     maintenance   operations    reprobe       start         unmanage      utilization
ban           constraints   help          manage        param         restart       status        unmigrate
crm(live)resource# move test_resource test3
INFO: Move constraint created for test_resource to test3
crm(live)resource# status
 test_resource	(ocf::heartbeat:IPaddr2):	Started
crm(live)resource# exit
bye
[root@test3 ~]# crm status
Last updated: Thu Jan  5 10:56:08 2017		Last change: Thu Jan  5 10:55:51 2017 by root via crm_resource on test3
Stack: classic openais (with plugin)
Current DC: test5 (version 1.1.14-8.el6_8.2-70404b0) - partition with quorum
3 nodes and 1 resource configured, 3 expected votes

Online: [ test1 test3 test5 ]

Full list of resources:

 test_resource	(ocf::heartbeat:IPaddr2):	Started test3



```

手工启动节点方法:

```
当某一个节点没有起来的时候,一般会自动加入,如果没有加入集群的话,我们可以手工把那个节点加入到集群里面:

crm(live)# node
crm(live)node# online test1
crm(live)node# online test3
然后查看状态即可
```

#### 注意事项

* 集群的所有节点必须在同一个子网内
* 配置各个节点的NTP时间同步




#### 参考链接

[crmsh]